---
layout: post
title: 设备虚拟化发展概述
date: 2021-10-17
tags: jekyll
---

## 简介

​		一台完整的计算机，离不开各种各样的外部设备，例如我们最长使用的鼠标，键盘，显示器，USB 等。同样的，虚拟机也需要模拟出各种类型的设备。在虚拟机里面使用的设备，例如网卡，磁盘，串口等，也都离不开设备虚拟化技术。这项技术也一直是云计算领域重要并且最基础的技术之一。

​		事实上，设备模拟的代码在 qemu 中，占了大部分。对于性能的追求，qemu得设备模拟也逐渐发展，经历了从传统的纯软件模拟，转向 virtio，再向 VFIO 直通设备的支持。下文将简单介绍设备虚拟化的发展过程。

## Trap-and-emulate

​		在最开始的阶段，qemu 能够完成用户程序模拟和系统虚拟化模拟。使用设备全模拟技术，模拟了所有真实设备的寄存器和操作流程。在这个过程中，qemu 能够模拟一个完整的系统虚拟机，该虚拟机有自己的 CPU，芯片组，虚拟内存和各种虚拟外部设备，能够为虚拟机中运行的操作系统和应用软件呈现出与物理计算机完全一致的硬件视图。但是很明显，访问虚拟机设备驱动的寄存器时，该指令会被陷出到 qemu，然后由 qemu 进一步处理，这样的操作带来严重的性能损耗。

## Virtio

​		在指令集模拟阶段，虚拟机环境中单网络和存储等，I/O 操作会完整地走内核协议站 -> qemu -> 宿主机内核栈，产生很多的 VM Exit和 VM Entry。设备在访问过程中，频繁地陷入陷出，带来严重的性能损耗。面对这样的问题。virtio 半虚拟化技术解决了这些问题。

​		virtio 重新定义了一套全新的框架，专门用于虚拟设备的驱动。虚拟设备在运行过程中，能够感知到自己处于虚拟化环境。并且会加载相应的 virtio 总线驱动和 virtio 设备驱动。例如磁盘设备使用 virtio-blk 驱动，网卡设备使用 virtio-net 驱动。

​		virtio 设备模拟也被称为半虚拟化。主要包括两个部分的内容：一个是 hypervisor 层创建出的模拟的设备，另一个是虚拟机内部操作系统安装好的模拟设备的驱动。virtio 实现了这个驱动和设备之间的对应通信接口。

​                                             		![emulate-and-virtio](https://developer.ibm.com/developer/default/articles/l-virtio/images/figure1.gif)

这张图展示了指令集模拟和 virtio 架构上的区别。virtio 是一种利用半虚拟化技术，提供 I/O 性能框架。

![virtio architecture](https://developer.ibm.com/developer/default/articles/l-virtio/images/figure3.gif)

​		virtio 是一种前后端架构，包括前段驱动和后端设备，以及自身定义的传输协议。例如上述的 virtio-blk，virtio-net 都是前端驱动(Front-End Driver)，virtio back-end drivers 是后端设备(Back-End Device)。

​		前端驱动：虚拟机内部所有 virtio 模拟设备都有对应的驱动才能正常运行。前端驱动主要作用是接收虚拟机用户态的请求，然后按照传输协议，将这些请求进行封装，再写入 I/O 端口，最后发送通知到 qemu 的后端设备。

​		后端设备：在 qemu 代码中实现，用来接收前端驱动发来的 I/O 请求，然后从接收的数据中按照传输协议的格式进行解析。例如网卡等需要实际物理设备交互的请求，后端驱动会对物理设备进一步操作，从而完成请求，最终通过中断机制通知前端驱动。

​		virtio 传输协议：virtio 前端和后端驱动的数据传输通过 virtio 环形队列完成。一个设备会对应若干个 virtio 队列，每个队列负责不同的数据传输。有点是控制层面的队列，有的是数据传输层面的队列。virtqueue 是通过 vring 实现的，vring 是虚拟机和 qemu 的一段共享内存，也是一段环形缓冲区。当虚拟机需要发送请求到 qemu 时，就准备好数据 buffer，放入一段共享内存，将 buffer 的数据描述内容放在 vring。通过写一个 I/O 端口，qemu 就能得到请求，从对应设备的 virtio 队列取出数据，进而从共享内存读到数据 buffer。qemu 在完成请求后，也将数据结构存放在 vring 中，前端驱动就可以从 vring 中得到数据。

​		基本原理如下图：

![vring](https://www.redhat.com/cms/managed-files/2020-07-08-virtio-fig6.png)

​		vring 包含三个部分，第一部分是描述符表(descirptor table)，用来描述 I/O 请求的传输数据信息，即保存上述共享内存 buffer 数据描述，包括共享内存起始地址，长度等信息。第二部分是可使用的 vring(available vring)，是前端驱动设置的表示后端设备可以使用的描述符表达起始索引。第三部分是已经使用的 vring(used vring)，是后端设备在使用完描述符表后设置的索引，这样前端驱动就可以知道哪些描述符已经被使用。vring 的本质也是实现一个无锁环形队列，在前后端数据传输过程中，不需要频繁锁住 descriptor table，再读写数据，提升前后端请求的性能。

## Vhost

​		在 virtio 驱动中，qemu 与物理网卡的数据通信，都是通过用户态读写 tap 设备完成的。这个过程涉及了虚拟机内核，宿主机 KVM，qemu 模拟，用户态陷入内核态，宿主机内核网络协议栈等多次转换。调用的路径很长，显然也带来了性能上的损失。vhost 技术的出现，就是对 virtio 框架下，qemu 后端切换到宿主机内核态的优化。当虚拟机陷入 KVM 之后，会直接在宿主机内核中进行收发包，不需要经过 qemu 宿主机用户态，显著提升了性能。

![vhost](https://access.redhat.com/webassets/avalon/d/Red_Hat_Enterprise_Linux-7-Virtualization_Tuning_and_Optimization_Guide-en-US/images/16a5072073f75500e9e21d6129f4056b/virtio_vhostnet.png)

​		vhost 是宿主机内核中的 vhost 模块，作为 virtio 的后端，替代了 virtio 驱动的 back-end driver。vhost 在接收来自虚拟机的通知后，直接在宿主机内核中与 tap 设备通信，完成网络数据包的收发。

​		通过将 virtio 数据面卸载到内核线程的方式，virtio 机制由原来的虚拟机驱动和 qemu 用户态 IO 线程通信，转为虚拟驱动与 vhost 内核 IO 线程通信。通过调用几个系统调用，就可以将 virtio 驱动的 virtqueue 地址，eventfd 等，向内核 vhost 驱动注册。所以，当 vhost 线程拿到虚拟机 virtio 驱动的数据后，直接在内核网络协议栈处理，优化了 qemu 切换到内核态这部分的开销。

## VFIO

​		随着云计算的不断发展，对设备的性能要求也越来越高。另一方面，对 GPU 这类设备的强烈需求。在这些背景下，VFIO 技术被提出来。VFIO 的全称是 virtio functio I/O，有人也戏称 very fast I/O。它是一套用户台设备驱动框架，能够将内核驱动呈现到用户态，并利用 IOMMU 机制，做到安全隔离，从而广泛运用在云计算这类由多组户需求的场景中。

​		VFIO 设备直通就是将物理设备直接挂到虚拟机，虚拟机通过直接与设备交互获得 I/O 性能的提升。传统上，设备驱动与设备进行交互需要访问设备很多资源，例如 PCI 设备配置空间，BAR地址空间，设备中断等。所有这些资源都在内核态进行分配和访问。在虚拟机环境下，把设备直通给虚拟机后，qemu 需要接管所有虚拟机对设备资源的访问。

​	VFIO 负责两部分的任务：第一个是将物理设备的各种资源分解，并将获取这些资源的接口向上导出到用户空间。第二部分是聚合，将从硬件设备得到的各种资源聚合起来，对虚拟机展示一个完整的设备接口。这种聚合是在用户空间完成。qemu 调用 KVM 接口将这些资源与虚拟机联系起来，使得虚拟机内部完全对 VFIO 存在无感知，虚拟机内部的操作系统能够透明地与直通设备交互，也能够正常访问直通设备的中断请求。

​		当虚拟机内，用户程序使用虚拟设备时，通过 EPT 技术，mmio 的访问会被重定向到实际的物理设备相应的bar空间，避免了陷出过程。在使用 VFIO 技术后，设备性能可以接近真实的物理设备。另外，VFIO 驱动利用 IOMMU 实现了设备 DMA 和中断重映射，起到的保护设备，安全隔离作用，也保证设备进行 DMA 时，通过给定的虚拟物理地址能访问到正确的物理内存。

​	                                                       	![IOMMU](https://pica.zhimg.com/80/v2-2fd7afc1e1898053f4671cd423bba909_720w.jpg?source=1940ef5c)

​		MMU，即内存管理单元，可以帮助 CPU 完成地址翻译，访问到正确的物理内存，另外也完成了对系统的物理内存资源进行管理。MMU 还可以对一些虚拟地址进行访问权限控制，以便于对用户程序的访问权限和范围进行管理，例如代码段。

​		IOMMU 的主要作用是将设备访问地址转换为存储器地址。通过 IOMMU 技术，可以对所有中断请求做一个重定向，从而将直通设备内中断正确分派到对应的虚拟机。VFIO 使用了 DMA rempping 功能，保证了虚拟机之间的地址隔离，Interrupt remapping 对中断正确分派。

## VFIO-mdev

​		VFIO 在应用场景，由一种限制是，一个设备只能透传给一个虚拟机，无法做到资源共享。SR-IOV 技术，从某种程度上能够解决这种问题，即将一个物理设备从硬件层面上进行资源划分，切分成多个 VF，然后分别透传给多个虚拟机使用。但是有很多设备，并不支持 SR-IOV 能力。因此，社区使用 VFIO-mdev 技术框架，希望提供一个标准接口，帮助设备驱动实现软件层面的资源切分，并仍然可以使用 VFIO 技术透传给虚拟机。

​		该技术实际上是在内核实现了一个虚拟设备(Mediated device)总线驱动模型，并在 VFIO 内核框架上进行扩展，增加了对 mdev 这类虚拟设备的支持(mdev bus driver)，从原来只支持从标准的硬件 PCI 设备获取透传信息(例如 PCI bar 空间)，变成既支持从硬件设备获取又可以从 mdev 设备驱动定义虚拟设备接口来获取。例如，当需要将一个 PCI 设备的 bar 空间作为资源切分的话，通过实现合适的 mdev 设备驱动，就可以将 bar 空间以 4KB (页面大小)为力度，分别透传给不同虚拟机使用。

## Vhost-user

​		VFIO 技术虽然能提供接近物理设备 I/O 的性能，但是该技术的缺点是不能热迁移。

​		另一方面，vhost 技术中，每个虚拟机都有对应的单独处理 I/O 的内核线程，其优化受到内核线程模型的制约。其次，用户态进程之间的通信，比如数据面的通信方案，openvswitch 和 SDN 的解决方案，guest 需要和 host 用户态的 vswitch 进行数据交换。如果采用 vhost 的方案，guest 和 host 之间又存在多次的上下文切换和数据拷贝

​		因此，提出了 vhost-user 的驱动方式。vhost-user 和 vhost 的实现原理是一样，都是采用 vring 完成共享内存，eventfd 机制完成事件通知。只是将 virtio 设备的数据面卸载到专用用户进程来处理。相比 vhost，vhost-user 实现是在用户空间中。

​		vhost-user 基于 C/S 的模式，采用 UNIX 域套接字（UNIX domain socket）来完成进程间的事件通知和数据交互，相比 vhost 中采用 ioctl 的方式，vhost-user 采用 socket 的方式大大简化了操作。只要 client 和 server 按照 vring 提供的接口实现所需功能即可，常见的实现方案是 client 实现在 guest OS 中，一般是集成在 virtio 驱动上，server 端实现在 qemu 中，也可以实现在各种数据面中。

​		vhost-user 使用专门的用户态进程，线程不受传统qemu和vhost线程模型制约，可以进行额外的优化。同时还可以通过1：M的方式，同时处理多个虚拟机的IO请求，相比vhost这类内核线程方式，用户进程在运维方面更加灵活。

![vhost-user](http://www.virtualopensystems.com/static/vosapp/images/guides/snabbswitch_qemu/images/vapp_client_server_arch.png)

## vDPA

​		VFIO 和 virtio 这两类技术一直是最主流的设备虚拟化技术。VFIO 能够直接将硬件资源透传给虚机使用，性能最佳，virtio 性能稍逊，但胜在更加灵活。

​		vDPA 的全称是 Virtio Data Path Acceleration，它表示一类设备：这类设备的数据面处理是严格遵循 Virtio 协议规范的，即驱动和设备会按照第三节提到的 Virtio 通信流程来进行通信，但控制路径，比如：通信流程里面提到的 ring buffer 和 descriptor table 的内存地址，驱动如何告知设备，设备支持的特性，驱动如何感知，这些都是厂商自定义的，不一定会遵循 Virtio 协议。这样做的好处是，可以降低厂商在实现这类设备时的复杂度。

​		Linux 内核为了将这类设备应用起来，就提出了 vDPA 这样一个技术框架。这个技术框架本质上和 VFIO-mdev 类似，也实现了一个虚拟设备（vDPA device）总线驱动模型，和 VFIO-mdev 不同的是，通过 vDPA 框架虚拟出来的设备，既可以给虚机使用，又可以直接从宿主机（比如：容器）进行访问。这一切都归功于，vDPA 设备的数据路径是遵循 Virtio 协议规范的，因此，可以直接被宿主机上的 virtio 驱动直接访问。同时，该技术框架对 vhost 内核子系统进行了扩展，赋予了类似 VFIO 技术框架的功能，允许将 vDPA 设备用来进行数据通信的硬件资源（ring buffer, descriptor table，doorbell 寄存器等）透传给虚机使用，这样，虚拟机的 virtio 驱动进行数据通信时，也是直接访问硬件资源，而不再需要通过 vhost、vhost-user 等方式进行处理了。更重要的一点是，由于虚机驱动是原本的 virtio 驱动，因此，当需要支持热迁移时，QEMU 可以灵活切换会软件模拟的方式，来保证热迁移的顺利进行。这样，vDPA 这个设备虚拟化技术框架既保证了最佳性能又保留了 virtio 设备的灵活性，而且还统一了虚机和容器的 I/O 技术栈。

![vDPA](https://www.redhat.com/cms/managed-files/2020-08-12-vdpa-kernel-fig3.jpg)

## 参考

1.https://developer.ibm.com/articles/l-virtio/

2.https://www.redhat.com/en/blog/virtqueues-and-virtio-ring-how-data-travels

3.https://www.cnblogs.com/bakari/p/8421743.html

4.https://mp.weixin.qq.com/s/g1kt0DDJwDk2Lg56R5suKw

5.https://www.linux-kvm.org/images/8/87/KVM17vDPA-v4_0.pdf

6.https://www.redhat.com/en/blog/introduction-vdpa-kernel-framework

7.http://www.virtualopensystems.com/static/vosapp/images/guides/snabbswitch_qemu/images/vapp_client_server_arch.png























